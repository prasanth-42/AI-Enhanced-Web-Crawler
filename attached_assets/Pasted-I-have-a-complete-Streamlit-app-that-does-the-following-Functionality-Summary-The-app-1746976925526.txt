I have a complete Streamlit app that does the following:

üîπ **Functionality Summary:**
- The app takes a **website URL** and scrapes its content using `WebBaseLoader`.
- It uses `HuggingFaceEmbeddings` + `Chroma` to embed and store the content.
- It runs a **Conversational RAG chain** using LangChain:
  - The model used is **Groq's LLaMA3-8b-8192** via the `groq` Python client.
  - The system is context-aware: it keeps `chat_history` to refine questions.
- The user interacts via a **chat interface** powered by Streamlit.
- It allows question answering based on the scraped site content using the LLM.

üîπ **Goal:**
I want to convert this Streamlit app into a **full-stack web app** with the following:

---

### üöÄ Frontend (React.js):
- A modern **chat UI similar to ChatGPT**:
  - Chat bubbles for messages (left/right for bot/user)
  - Input box at the bottom
  - Spinner or loading indicator during scraping/response
- Title at the top: `AI Web Crawler`
- Logo at top-left: **crawler/spider icon**
- Allow user to input a **website URL** and see a loading state while it is scraped
- High-tech, cyberpunk/digital-themed UI:
  - Dark mode
  - Neon blue, green, or purple accents
  - Sleek, futuristic font

---

### üîß Backend (Flask, Python):
- Flask server exposes two routes:
  - `POST /scrape`: receives a URL, scrapes and splits content using `WebBaseLoader` and `RecursiveCharacterTextSplitter`, stores in ChromaDB with `all-MiniLM-L6-v2` embeddings
  - `POST /chat`: receives user query and chat history, responds using a RAG pipeline:
    - Create retriever with LangChain
    - Use custom `GroqChatLLM` class (already implemented)
    - Return the generated LLM response

- Please reuse the logic I already implemented in my Streamlit version including:
  - Custom Groq integration
  - Conversational chain using `create_history_aware_retriever` and `create_retrieval_chain`

---

### üìÅ Project Structure:
Organize into:
- `/client`: React frontend
- `/server`: Flask backend with LLM and scraping logic
- `.env` support for storing `GROQ_API_KEY`

---

### ‚úÖ Bonus:
- Please configure the frontend and backend to talk to each other properly (handle CORS or use proxy).
- Add easy-to-understand comments.
- If needed, ask API KEY 

---

Please generate all necessary files and structure it so I can deploy or run on Replit smoothly. You can replace the actual LLM call temporarily with a placeholder function if required for testing.